{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMv6GXeqmU5ygTnkGJNFfVV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinayak2019/ml_for_molecules/blob/main/Pre_trained_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using pre-trained models\n",
        "\n",
        "If the model parameters and optimized weights are available, the model can be used to make predictions or train further (transfer learning)\n",
        "\n",
        "Here, we will use the graph neural network from the published [article](https://pubs.rsc.org/en/Content/ArticleLanding/2022/SC/D2SC04676H). We will the the molecular property lowest singlet excitation which could correspond to absorption maxima.\n",
        "\n",
        "First, we will download the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "7uJHI0-LcUBY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qff48u-Pat1R"
      },
      "outputs": [],
      "source": [
        "# download the parameters and weights\n",
        "! wget https://data.materialsdatafacility.org/mdf_open/ocelotml_2d_v1.2/s0t1_3gen/best_r2.pt\n",
        "! wget https://data.materialsdatafacility.org/mdf_open/ocelotml_2d_v1.2/s0t1_3gen/params.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install the packages\n",
        "! pip install dgl\n",
        "! pip install dgllife\n",
        "! pip install rdkit"
      ],
      "metadata": {
        "id": "qSQIJTQNcmti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the code for the MPNN used in the article"
      ],
      "metadata": {
        "id": "7SDrfMmc2fAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from dgl.nn.pytorch import Set2Set\n",
        "from dgllife.model.gnn import MPNNGNN\n",
        "\n",
        "\n",
        "class MPNN_readout(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 node_in_feats,\n",
        "                 edge_in_feats,\n",
        "                 node_out_feats=64,\n",
        "                 edge_hidden_feats=128,\n",
        "                 n_tasks=1,\n",
        "                 num_step_message_passing=6,\n",
        "                 num_step_set2set=6,\n",
        "                 dropout=0,\n",
        "                 num_layer_set2set=3, descriptor_feats=0):\n",
        "        super(MPNN_readout, self).__init__()\n",
        "\n",
        "        self.gnn = MPNNGNN(node_in_feats=node_in_feats,\n",
        "                           node_out_feats=node_out_feats,\n",
        "                           edge_in_feats=edge_in_feats,\n",
        "                           edge_hidden_feats=edge_hidden_feats,\n",
        "                           num_step_message_passing=num_step_message_passing)\n",
        "        self.readout = Set2Set(input_dim=node_out_feats,\n",
        "                               n_iters=num_step_set2set,\n",
        "                               n_layers=num_layer_set2set)\n",
        "        self.predict = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(2 * node_out_feats + descriptor_feats, node_out_feats),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(node_out_feats),\n",
        "            nn.Linear(node_out_feats, n_tasks)\n",
        "        )\n",
        "\n",
        "    def forward(self, g, node_feats, edge_feats, concat_feats=None):\n",
        "        node_feats = self.gnn(g, node_feats, edge_feats)\n",
        "        graph_feats = self.readout(g, node_feats)\n",
        "        if concat_feats != None:\n",
        "            final_feats = torch.cat((graph_feats, concat_feats), dim=1)\n",
        "        else:\n",
        "            final_feats = graph_feats\n",
        "        return self.predict(final_feats)"
      ],
      "metadata": {
        "id": "3dNFACCEb-px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the model parameters from the params.json file"
      ],
      "metadata": {
        "id": "vV7n6t1Silab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"params.json\") as f:\n",
        "  params = json.load(f)\n",
        "\n",
        "params"
      ],
      "metadata": {
        "id": "-gYpAodcipzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to add the node_in_feats which is the length of CanonicalAtomFeatures , 74 and edge_in_feats is 12 from the CanonicalBondFeaturizer."
      ],
      "metadata": {
        "id": "VevY3oi7i2fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params.update(\n",
        "    {\n",
        "      \"node_in_feats\" : 74,\n",
        "      \"edge_in_feats\": 12\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "d6II6iBIjIVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create the model object"
      ],
      "metadata": {
        "id": "ROty_exH2m0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MPNN_readout(**params)\n",
        "model"
      ],
      "metadata": {
        "id": "VTO8PJoHci4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the model parameters set. But we don't have the optimized weights. Let's load the weights from the pre-trained model."
      ],
      "metadata": {
        "id": "EW5A_Bsgjj4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"best_r2.pt\", map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "u448Qag4jU2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the graphs for input to model"
      ],
      "metadata": {
        "id": "gJIkoxuzyo6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import from rdkit and dgl-lifesci\n",
        "from rdkit import Chem\n",
        "from dgllife.utils import CanonicalAtomFeaturizer, CanonicalBondFeaturizer, \\\n",
        "mol_to_bigraph\n",
        "\n",
        "# create the atom and bond featurizer object\n",
        "atom_featurizer = CanonicalAtomFeaturizer(atom_data_field=\"hv\")\n",
        "bond_featurizer = CanonicalBondFeaturizer(bond_data_field=\"he\")\n",
        "\n",
        "# example smiles - ethane\n",
        "smiles = \"CC\"\n",
        "\n",
        "# mol_to_graph requires the RDKit molecule and featurizers\n",
        "mol = Chem.MolFromSmiles(smiles)\n",
        "graph = mol_to_bigraph(mol, node_featurizer=atom_featurizer, \n",
        "                     edge_featurizer=bond_featurizer)\n",
        "\n",
        "# display the graph object\n",
        "graph"
      ],
      "metadata": {
        "id": "tKbalr_0yp51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make predictions"
      ],
      "metadata": {
        "id": "lGWQ2y01y2qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "node_feats = graph.ndata[\"hv\"]\n",
        "edge_feats = graph.edata[\"he\"]\n",
        "model(graph, node_feats, edge_feats)"
      ],
      "metadata": {
        "id": "Mg5z5D65yu8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving a trained pytorch model\n",
        "\n",
        "Use the `torch.save` function and pass in the model state_dict and name"
      ],
      "metadata": {
        "id": "d859y_rizuL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"my_model.pt\")"
      ],
      "metadata": {
        "id": "tUa5JXdTzwOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer learning\n",
        "\n",
        "When data is limited for prediction one task, a model that is trained on another task with large data can be used to produce models with higher accuracy than starting model training from scratch.\n",
        "\n",
        "Let's say there is a model to predict HOMO-LUMO gap trained on the QM9 dataset. If your task is to now predict the HOMO energies, there is no need to start the model training from scratch. You can use the optimized weights from the HOMO-LUMO gap predictor and not change them for inner layers. Only the weights for the penultimate layers could be optimized for the HOMO energy prediction model. This process is called transfer learning.\n",
        "\n",
        "Here, we will freeze the `gnn` and `readout` layers and allow the weights on the `predict` layer to be trainable."
      ],
      "metadata": {
        "id": "T60HYWG5LNBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "v41sy1VJ4H54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the parameter (weights)"
      ],
      "metadata": {
        "id": "MEc4zzCBMfMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  print(param)"
      ],
      "metadata": {
        "id": "JyP1VxVx4Um9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see all have gradients. To freeze the weights we need to remove the gradients "
      ],
      "metadata": {
        "id": "M-p6fr8ZMjXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "4QX9QfYx27OT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All weights are frozen. If the model is used for training, the model weights will not change. This implies not learning."
      ],
      "metadata": {
        "id": "2doOj1rDMtkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  print(param)"
      ],
      "metadata": {
        "id": "797IXvQJ28in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want the predict layer weights to be trainable. Let's not freeze those"
      ],
      "metadata": {
        "id": "wueZKg1jM3Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.predict.parameters():\n",
        "  param.requires_grad = True"
      ],
      "metadata": {
        "id": "v-2AekD64ED3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the weights again"
      ],
      "metadata": {
        "id": "YzDNf1FDNB0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  print(param)"
      ],
      "metadata": {
        "id": "7xDabw0Z45fF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}