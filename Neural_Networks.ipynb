{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvHufoxOLZ0ZoVfxfARVvT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinayak2019/ml_for_molecules/blob/main/Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overview\n",
        "\n",
        "We will use pytorch for training our neural networks. First, we will look at the overall pipeline to train and validate the model. For this, we will use ML models from already available python pacakges like ``dgl`` and ``deepchem``. \n",
        "\n",
        "\n",
        "The process of training and validating model is below --\n",
        "\n",
        "1. Clean up dataset\n",
        "2. Featurize the data\n",
        "3. Split the dataset\n",
        "4. Create ``DataLoader`` for the dataset splits\n",
        "5. Create the ML model, define loss function and optimizer\n",
        "6. ``for`` loop for epochs\n",
        "    1. ``for`` loop for training batches\n",
        "        1. Do a forward pass\n",
        "        2. Compute the loss\n",
        "        3. Do backpropogation\n",
        "    2. ``for`` loop for validation batches\n",
        "        1. Do a forward pass\n",
        "        2. Compute the loss\n",
        "    "
      ],
      "metadata": {
        "id": "lOVwv8YkB7Pd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing the packages"
      ],
      "metadata": {
        "id": "5658_JEJnxfS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HC4Yn8cLAaC9"
      },
      "outputs": [],
      "source": [
        "# install deepchem, dgl, rdkit and fast-ml\n",
        "! pip install deepchem\n",
        "! pip install dgl\n",
        "! pip install dgllife\n",
        "! pip install rdkit\n",
        "! pip install fast_ml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset operations\n",
        "\n",
        "We will fetch the QM9 dataset and use HOMO-LUMO gap as the target for prediction. "
      ],
      "metadata": {
        "id": "136hy3mDHYFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas library\n",
        "import pandas as pd\n",
        "\n",
        "# load the dataframe as CSV from URL. \n",
        "df = pd.read_csv(\"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/qm9.csv\")\n",
        "\n",
        "# create the dataset with smiles and gap\n",
        "dataset = df[[\"smiles\",\"gap\"]].sample(frac=0.1)"
      ],
      "metadata": {
        "id": "_9KYkMU1FJ3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For featurizing the SMILES, we will use ``CircularFingerprint`` from deepchem."
      ],
      "metadata": {
        "id": "rs3sQHgOoh1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import depechem and rdkit\n",
        "import deepchem as dc\n",
        "from rdkit import Chem\n",
        "\n",
        "# create the featurizer object\n",
        "featurizer = dc.feat.CircularFingerprint(radius=2, size=100)\n",
        "\n",
        "# apply the featurizer to dataset\n",
        "dataset[\"fp\"] = dataset[\"smiles\"].apply(featurizer.featurize)\n",
        "\n",
        "# the fp is an array; we will convert it to a list as required for model input \n",
        "dataset[\"fp\"] = dataset[\"fp\"].apply(lambda x: list(x[0]))\n",
        "\n",
        "# just use the fp and gap part of the dataset\n",
        "dataset = dataset[[\"fp\",\"gap\"]]"
      ],
      "metadata": {
        "id": "M05vMWS6FT0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we use random splitting with Fast-ML. Other splitters could also be used."
      ],
      "metadata": {
        "id": "eOhc1WzunuEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the function to split into train-valid-test\n",
        "from fast_ml.model_development import train_valid_test_split\n",
        "\n",
        "X_train, y_train, X_valid, y_valid, \\\n",
        "X_test, y_test = train_valid_test_split(dataset, target = \"gap\", train_size=0.8,\n",
        "                                        valid_size=0.1, test_size=0.1) "
      ],
      "metadata": {
        "id": "0Uz09CqxF-2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the dataset before procceding."
      ],
      "metadata": {
        "id": "wEgPjvkypgp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.head()"
      ],
      "metadata": {
        "id": "BE5KuSqnHOsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataloader\n",
        "\n",
        "The ``DataLoader`` helps in creating batches, shuffling data and feeding the data into to model during training. The dataloader requires the dataset in the form (X,y) where X is the input and y is the target.\n",
        "\n",
        "The dataloader code below does this transformation. The ``collate_data`` function is need for batching the (X,y) entries before feeding the batches into the model."
      ],
      "metadata": {
        "id": "fu7uSLoJKfjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_data(data):\n",
        "  # our data is in the form of list of (X,y)\n",
        "  # the map function thus maps accordingly\n",
        "  X, y = map(list, zip(*data))\n",
        "\n",
        "  # we need to stack the Xs and ys for different entries in the batch\n",
        "  X = torch.stack(X, dim=0)\n",
        "  y = torch.stack(y, dim=0)\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "wNAZGrMuY8yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dataloader\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# create the dataloader for train dataset\n",
        "# dataset should be of form (X,y) according to the collate function\n",
        "# the inputs should also be converted to tensors\n",
        "train_dataloader = DataLoader(\n",
        "    dataset=list(zip(torch.tensor(X_train[\"fp\"].values.tolist(), dtype=torch.float32),\n",
        "                     torch.tensor(y_train.tolist(), dtype=torch.float32))),\n",
        "    batch_size=64, collate_fn=collate_data)"
      ],
      "metadata": {
        "id": "QG1Ie35wJsiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look at the first entry of the train_dataloader with -"
      ],
      "metadata": {
        "id": "7b73yfdorVEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader.dataset[0]"
      ],
      "metadata": {
        "id": "1gxYN8NYL8P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat the same for the valid_dataset and test_dataset"
      ],
      "metadata": {
        "id": "9kv-d9Y0ru8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataloader = DataLoader(\n",
        "    dataset=list(zip(torch.tensor(X_valid[\"fp\"].values.tolist(), dtype=torch.float32),\n",
        "                     torch.tensor(y_valid.tolist(), dtype=torch.float32))),\n",
        "    batch_size=64, collate_fn=collate_data)\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset=list(zip(torch.tensor(X_test[\"fp\"].values.tolist(), dtype=torch.float32),\n",
        "                     torch.tensor(y_test.tolist(), dtype=torch.float32))),\n",
        "    batch_size=64, collate_fn=collate_data)"
      ],
      "metadata": {
        "id": "CfmWl76dKpl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model, loss and optimizer\n",
        "\n",
        "ML models architectures are already available in many python packages including deepchem, dgl-lifesci, chemprop, chemml.\n",
        "\n",
        "Here, we will use a model from dgl-lifesci package to show the process of training a ML model. deepchem has a streamlined process and you cannot see what happens behind the scene. We will look at one such implementation later."
      ],
      "metadata": {
        "id": "-fotYO2ZHhvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import MLP model from dgl-lifesci\n",
        "from dgllife.model.model_zoo.mlp_predictor import MLPPredictor\n",
        "\n",
        "model = MLPPredictor(in_feats=100, hidden_feats=512, n_tasks=1, dropout=0.)\n",
        "model"
      ],
      "metadata": {
        "id": "AhGhxXPdHQR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ``MLPPredictor`` is a simple two layer model with dropout, batchnorm and ReLU activation. This alone may not be a good model, but is inexpensive for training demonstration.\n",
        "\n",
        "Once the model is created, we can set the loss function"
      ],
      "metadata": {
        "id": "uEG7a44fp27c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function for regresssion is usually mean squared error\n",
        "import torch\n",
        "\n",
        "loss_func = torch.nn.MSELoss(reduce=None)"
      ],
      "metadata": {
        "id": "rtzhleqWJHUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the Adam optimizer for training."
      ],
      "metadata": {
        "id": "HDWEFt02p8X9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adam optimier\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "KCe6e4l4Jjft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model training and validation\n",
        "\n",
        "We follow the steps in the overview"
      ],
      "metadata": {
        "id": "Ewu1-wIYMPeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "\n",
        "# loop over epochs\n",
        "for epoch in range(epochs):\n",
        "  print(\"\\nStarting Epoch\", epoch+1)\n",
        "\n",
        "  model.train()\n",
        "  # loop over training batches\n",
        "\n",
        "  train_loss = []\n",
        "  for batch in train_dataloader: \n",
        "\n",
        "    # Do a forward pass\n",
        "    feats, target = batch\n",
        "    predictions = model(feats)\n",
        "  \n",
        "    # Compute loss and gradient\n",
        "    loss = (loss_func(predictions, target)).mean()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Do back propogation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # save loss to compute average loss\n",
        "    train_loss.append(loss)\n",
        "\n",
        "  print(\"Training loss\", torch.tensor(train_loss).mean().item())\n",
        "\n",
        "\n",
        "  # loop over validation batches\n",
        "  model.eval()\n",
        "  valid_loss = []\n",
        "  with torch.no_grad():\n",
        "    for batch in valid_dataloader:\n",
        "      \n",
        "      # Do a forward pass\n",
        "      feats, target = batch\n",
        "      predictions = model(feats)\n",
        "    \n",
        "      # Compute loss and gradient\n",
        "      loss = (loss_func(predictions, target)).mean()\n",
        "      valid_loss.append(loss)\n",
        "  print(\"Validation loss \", torch.tensor(valid_loss).mean().item())\n"
      ],
      "metadata": {
        "id": "is7r-fWuKYMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the performance\n",
        "\n",
        "We can get a random sample from the test dataset and look at the predicted and true value"
      ],
      "metadata": {
        "id": "GznlVwBCqIKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 159\n",
        "x_sample = X_test[\"fp\"].iloc[idx]\n",
        "y_sample = y_test.iloc[idx]\n",
        "\n",
        "y_sample"
      ],
      "metadata": {
        "id": "VLPTF8QYN-tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model(torch.tensor([x_sample], dtype=torch.float32))"
      ],
      "metadata": {
        "id": "_rwTE1NDYW8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us get prediction over the entire test dataset"
      ],
      "metadata": {
        "id": "pAmzHCFPqMsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_values = []\n",
        "true_values = y_test.to_list()\n",
        "\n",
        "model.eval()\n",
        "for sample in X_test[\"fp\"].tolist():\n",
        "   prediction = model(torch.tensor([x_sample], dtype=torch.float32))\n",
        "   predicted_values.append(prediction.item())"
      ],
      "metadata": {
        "id": "YiRQBhA-hHEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create a scatter plot to look at the correlation"
      ],
      "metadata": {
        "id": "2Dm2_Ztcqa9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(true_values, predicted_values)"
      ],
      "metadata": {
        "id": "_uENBx8EqdpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we noted before, the model predictions are not good; predicts nearly constant value. We can also arrive at the conclusion from the R<sup>2</sup> score"
      ],
      "metadata": {
        "id": "_lfgLQO2qSMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "print(\"R2 score \", r2_score(true_values,predicted_values))"
      ],
      "metadata": {
        "id": "E_VCh7Qti15_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}